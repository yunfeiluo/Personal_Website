{
    "id": -1,
    "docs": [],
    "type": "report",
    "title": "Student Stress Prediction, Machine Learning Research Project",
    "tags": ["Machine Learning", "Artificial Intelligence", "Open Source", "Neural Networks"], 
    "path": "articles/reports/studentlife_spring2020_report.pdf",
    "summery": "Open source project on predicting student's stress with Multitask Learning and Time series data. The Dataset, StudentLife, was collected by a program in Dartmouth College.",
    "text": "Independent Study Report\nUniversity of Massachusetts Amherst\nInstructors: Madalina Fiterau, Iman Deznabi\nStudent: Yunfei Luo\nSpring 2020\nMay 12, 2020\n||||||||||||||||||||||||||||||||||||||||\nPersonalized Student Stress Prediction with Deep Multitask Network\nAbstract\nIn this semester, I'm working on predicting student stress with putting students in groups, in order\nto explore the possibility to make the original personalized model more applicable. Though it is\nhard for the \"grouplized\" model to have better performance than the personalized model, my ex-\nperiments shows that grouping students by the average stress level could have F1 score that is really\nclose to the score achieved by the personalized model.\n1. Introduction and Previous work\nThe previous state-of-the-art model is called Cross-Personal Activity LSTM Multitask Auto-Encoder\nNetwork (CALM-Net), see Personalized Student Stress Prediction with Deep Multitask Network by\nAbhinav Shaw, Natcha Simsiri, Iman Deznabi, Madalina Fiterau, Tauhidur Rahman.\nThe model is constituted by a Long Term Short Memory (LSTM) autoencoder connected by fully\nconnected layers, then connected by sub multi-layer-perceptrons (MLP) for each students. I'm\nmainly working on the sub MLP part.\nThe main problem is that when we have a new student, without collecting some data from the\nstudent, it is hard for the personalized model to make prediction due to the lack in training data.\nSo for the sub MLP, instead of building them for each student, we build them for each group of\nstudents. So when a new student come, instead of collecting data from the student, we just need\nto collect some features of the students, then we could put the student into the group that contains\nstudents who share similar characteristics with the new student, then make prediction.\n\nFigure 1.Personalized Model\nFigure 2.Grouplized Model\n2. Background Information\nThe StudentLife dataset was conducted in Dartmouth college where passive sensing and survey data\nwas collected over 10 weeks among 48 students.\n3. Methods\n3.1. Density Based Clustering based on Dynamic Time Warping (DTW) on series of stress labels\nThe stress labels we have for each student form a time series data. Since the stress labels for\n\nthe students falls in different set of days during the 2 months range, using distance criteria like Eu-\nclidean and Manhattan distance is not reasonable. In order to find the similarity among students,\nstudents can be clustered based on the DTW distance, that considering the distortion of the data.\nSince each student have different number of stress label recorded, the lengths of these time se-\nries data are different. So the methods based on finding centroids such K-Means and Mean-shift are\nnot applicable. Instead, hierarchical clustering methods can be used in this case. Density-Based-\nSpatial-Clustering-of-Applications-with-Noise (DBSCAN) is the one I used for clustering these series\ndata. The noise data points were greedily assigned to the existed groups that are the closest to them.\nFigure 3. samples of clusters.\nThe x-axis is the time (days in a year + (hours in the day / 24)), the y-axis is the stress label\n\n3.2. K-Means Clustering on aggregated stress labels, i.e. average stress labels\nConsidering that the density based clustering methods based on DTW is a bit complex, I sim-\nply aggregated the series data with their means to be the data point. Then the clustering becomes\nmuch easier. Though I use K-Means for clustering, it is nothing but put thresholds to split the data\npoints.\nFigure 4. clusters visualize, where x-axis is the average stress labels\n3.3. Density Based Clustering on chosen features from aggregated surveys score\nThe ideal way is to clustering students based on surveys score. So for a new student, we could\nfirst collect the survey results, then put the student in to proper cluster. For now, I had use the\nfeatures: average hours slept, average deadline per week, and mode sleep rating. In order to guar-\nantee that in the training dataset, no group contains only one student, I use the same methods in\n3.1: use DBSCAN first, then put the noise data in the closest clusters.\n\nFigure 5. Clusters visualize with three chosen features from surveys\n4. Experiment results:\nThe model evaluation method I used is 5-fold cross validation. The metrics include F1 score and\nAUC were averaged across the 5 splits. I've also include the results of original data in the table, i.e.\neach group contain exactly one student.\nTable 1. The best results that each different clustering method obtained\n5. Conclusion and Future work\nThe results from experiments shows that using clusters based on average stress labels has better\n\nperformance. However, in order to know how well a model performed, we need to run leave one out\nvalidations, which haven't being completed. After that, we could know how the model generalized\namong each individual.\nIn addition, there are still many other surveys scores that haven't been used in clustering. In\norder to use these features, I need to do more work on knowing how the scores of different types of\nsurveys are aggregated.\nA. Appendix\nA.1. Model Configuration\nWeights for Losses: alpha = 0:0001 for autoencoder reconstruction error, beta = 1 for classi\fcation error\nAuto-Encoder bottleneck size: 128\nShared Layer size: 256\nUser(Group) Layer size: 64\nEpochs: 500\nGradient Descent step size: 0.000001\nL2 norm regularization coeff: 0.0001\nDropout probabilities: None\nValidation method: 5-fold validation, strati\fed splitting by the group-id/stress-label pairs\n"
}