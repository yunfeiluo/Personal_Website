<<<<<<< HEAD
{
    "id": -1,
    "type": "blog",
    "title": "Introduction to functional Magnetic Resonance Imaging",
    "path": "articles/blogs/Intro_to_fMRI.pdf",
    "docs": [],
    "tags": [
        "Cognitive Science",
        "Machine Learning",
        "Psychology",
        "Physics",
        "Biology",
        "Neural Science"
    ],
    "summery": "With the advanced technologies of Magnetic Resonance Imaging (MRI) and Statistical Analysis, scientists are able to study human's brain in an appropriate way. This blog provides a clear and comprehensive representation of the bird-view of fMRI field based on my personal learning note of the course, Principles of fMRI 1 and 2}, offered by Johns Hopkins University and University of Colorado Boulder.",
    "text": "With the advanced technologies of Magnetic Resonance Imaging (MRI) and Statistical Analysis, scientists are able to study human's brain in an appropriate way. In the fields of Cognitive Science, Neural Science, and Psychology, studying the relationships between the brain activity and the behaviors or psychological states reveals great potentials in clinical applications. With the inclusion of techniques in Computer Science, the scientific studies becomes more effective, which can be revealed in the interdisciplinary field: functional Magnetic Resonance Imaging (fMRI), that dealing with series of MRI images across time. This blog provides a clear and comprehensive representation of the bird-view of fMRI field based on my personal learning note of the course \\textit{Principles of fMRI 1 \\& 2} offered by Johns Hopkins University and University of Colorado Boulder. \nThe final goals of fMRI can be summarized as:\n\\\\ \\\\\ni) \\textbf{Localization}: Determine which parts of the brain are activated during specific task(s).\n\\\\ \\\\\nii) \\textbf{Connectivity}: How regions are connected with one another. \n\\\\ \\\\\niii) \\textbf{Prediction}: Use brain activity to predict perceptions, behavior, or health status. \n\\\\ \\\\\nBefore we analyze toward the interests above, we first need to design an appropriate experiment to approach the tasks that we are interested in. Block Design and Event-related Design are the two main types of experimental design which will be discussed in details in the following sections. \n\\\\ \\\\\nThen we need to collect data, and analyze the data. The proper analyzing modes are Tradition Brain Mapping, which is a classical approach in science, and Multivoxel (or Multivariate) Pattern Analysis (MVPA) which is often related to techniques of Machine Learning. These modes will be discussed in details in the later sections. \n\\\\ \\\\\nMoreover, considering that there are many variability in experimental design, model selection, and the varies source of noises such as head movements, heart rate, and respiration, etc., the quality control is significant across the entire fMRI project as shown in Figure 1. Again, more details will be discussed in the later sections. \n\n\\section{Experimental Design}\nDuring the experiments, we want the subjects to do the tasks we want, and detect the effects based on the observations on the fMRI data of the brain. For example, we may want to see how brain responses if we ask the participants to look at images that could be either positive (make people feel comfortable, could be happy or enjoyment) or negative (make people feel bad, such as sad and pain). The question is now revealed: how should we arrange the order of tasks that exposes to or done by the subjects in order to effectively detect the contrasts or responses from the brain activity? The two main strategies are Block-Design and Event-Related-Design, as shown in Figure 2. \n\n\\subsection{Block Design}\nIn Block design, the similar events are grouped into a single block. The length of the block is fixed once set during the experiment. \n\\\\ \\\\\nEach blocks have the same length, often about 16 to 20 seconds. If the block length is too short, there might not have enough time space for the brain to show differences among different tasks, which often caused by vascular inelasticity. If use long blocks, more than 40 seconds for example, it is risky that there will be more unwanted activation caused by the out-of-scope association and imagination done by the subjects. \n\n\\subsection{Event-Related Design}\nIn this design mode, the tasks are mixed in orders, with unequal length. \n\\\\ \\\\\nFor this kind of design, there are no strict requirements on the boundaries of the events' length, but if there are low-frequency components, such as a long gap between two events, it is still risky with the same reasons as mentioned in Block-Design. \nThe Rule-of Thumb is to have 30 to 40 minutes of functional scan time for each subjects considering the psychological effects such as fatigue and habituation. And scan as many participants as possible for better generalization and robustness of the study. \n\\\\ \\\\\nThe tradeoffs among different factors always exist. The block design is robust in detect the contrast of responses of brain among different tasks. However, the blocks always comes in same length and same order. Considering the effects of Neural Habituation, i.e. the subjects will know what will be the next block, and will be subconsciously prepared for that, the contrast results is not sufficient for making specific psychological or behavioral inferences. On the other hand, although event-related design might not provide a clear contrast in responses among the tasks, it has better estimation to the shape of hemodynamic responses which is significant for making specific inferences. \n\\\\ \\\\\nIn terms of efficiency, having 2 conditions (2 different tasks) is optimal in either design mode. And the experimental efficiency has order: \\\\\nBlock-Design $>$ Dense-Event-Related-Design $>$ Sparse-Event-Related-Design\n\\\\ \\\\\nIn the cases where the number of conditions $\\ge 2$, Genetic Algorithm (GA) is often used for searching in the random space of the order of tasks exposes to the subjects. Details of GA will not be discussed here. Basically an MRI images consist of number of voxels with their value of intensity, which can also be interpret as the pixels in 3 dimensions. So in fMRI, we have a time series data for each voxel as shown in Figure 4. \n\n\\subsection{Data Acquisition}\nPhysically, the data is collected by controlling the motion of nuclei of hydrogen atoms of the subjects. Each nuclei is spinning at some directions, and has its own magnetic field. When the subjects is placed in the Magnetic Resonance (MR) scanner, the nuclei will align with the magnetic field of the equipment and form an \\textbf{Longitudinal Magnetization}. The nuclei now have the same directions of spinning, but could have different phases. \n\\\\ \\\\\nThen during the scanning, a Radio Frequency (RF) pulse is performed. The phases of the nuclei will be aligned, and will be tipped over and form an \\textbf{Transversal Magnetization}. The degree for such tip-over is often 90 degrees. \n\\\\ \\\\\nThen we remove the RF pulse, and the transversal Magnetization will decrease exponentially, and longitudinal magnetization will increase exponentially back to its original size. The signal created during this process will then be catched by a receiver coil. The signal is then map to a image as shown in Figure 3 by Fourier Transfer. \n\n\\subsection{Blood Oxygenation Level Dependent (BOLD)}\nStudying oxygenation changes in the brain across time is the most common approach in fMRI. It measures the ratio of oxygenated to deoxygenated hemoglobin in the blood. Because oxyhemoglobin is diamagnetic and deoxyhemoglobin is paramagnetic, their difference in magnetism allow us to catch the signal of how these two types of hemoglobin distributed and changes in the brain. \n\\\\ \\\\\nSince when the neurons are activated, they will absorb oxyhemoglobin from the neighbor blood vessels, we doesn't measure the neural activity directly, but metabolic demands (oxygen consumption) of the active neurons. As the neurons absorb oxyhemoglobin, there will be a quick decrease in oxyhemoglobin and increase in deoxyhemoglobin, which will be shown as an decreases of magnetic resonance signal. But suddenly, as the body will sending more oxyhemoglobin to reply the oxygen consumption, there will be a significant increase in oxyhemoglobin and decrease in deoxyhemoglobin, which will result in more magnetic resonance signal. After about 4 to 6 seconds, the intensity of the signal will reach the peak, and start reduces below the baseline, then go back to the original situations. The entire process demostrate an delayed responses and is quite slow that it would take about 20 to 30 seconds. The signal vs. time wave is known as Hemodynamic Response Function (HRF), which we will discuss more about it in the section of analysis. \n\n\\subsection{Noise \\& Registration}\nThere are always noise during the scanning, could be artifacts or errors that are inevitable. Head motion, heart beat, and respiration are often great concerns in fMRI. In order to dealing with such noise, an Intrasubject registration is needed, where we need to ensure that each voxels across time always represent the same area. However, the motion often influence the the magnetic field which will leave a \"spin-history\" of the nuclei, which cannot be entirely removed. \n\\\\ \\\\\nMoreover, for the purpose of robustness and generalization of the study, a spatial normalization is needed, which often refer to as Intersubject registration. One subject will often be selected as the \"standard\", and the data from other subjects will be manipulated to align with the standard. Such normalization allows researchers to come up with the results making sense across the population. \n\n\\subsection{Quality Control}\nAs we mentioned in the previous section, there are always tradeoffs that need to deal with. Echo-Planer Imaging (EPI), a standard and widely used techniques for obtaining functional images, collects images slice-by-slice from bottom to top of the brain. Figure 5 briefly shows the tradeoff relationships of EPI among brain coverage, Spatial and Temporal resolution, and susceptibility artifacts. \nFirstly, more coverage of the brain will help a lot in registration and normalization, but there will be less temporal resolution and more susceptibility artifacts. Secondly, although higher resolution often reduce the susceptibility artifacts, spatial and temporal resolution always cannot be compatible. Higher spatial resolution lead to better decoding and prediction, especially in small regions activity. But there are cost in temporal resolution. Higher temporal resolution has better ability to separate and remove artifacts but has cost in spatial resolution, time, and coverage. Finally, higher susceptibility artifacts will provide better fidelity, especially localization in problematic brain areas.\nUsually, the analyzing tasks use mass univariate approach, which is also known as traditional brain mapping, as shown in Figure 6. In this approach we modeling for each voxel separatly and independently.\nFirst of all, we will fit a General Linear Model (GLM) for each voxel as shown in Figure 7 with: \n$$Y=X\\beta+\\epsilon$$\nWhere $Y$ is the observed time series data of the single voxel. $X$ is the design matrix, which is often the assumed canonical HRF, or matrix that contain canonical HRF, its derivative, and its dispersion derivative. Finite Impulse Response (FIR) is also a proper choices for the design matrix. $\\beta$ is the regressors, and $\\epsilon$ is the error term. By perform the fitting task, we are solving the optimization problem:\n$$min_{\\beta} \\;\\; ||Y-X\\beta||_2^2$$\nWe can easily solve this problem by taking the derivative with respect to $\\beta$ and set it to zero. We finally arrive the Ordinary Least Squares (OLS):\n$$\\beta^* = (X^T X)^{-1}X^T Y$$\nWhere the diagnal of the matrix $(X^T X)^{-1}$ is defined as the design efficiency, which is a significant metric to evaluate the tradeoffs. However, OLS is only optimal if the error or the noise term $\\epsilon$ is i.i.d (Independent and Identical Distributed). Otherwise, we will introduce a weighted matrix $W$ that corresponds to the actual variance:\n$$\\beta^* = (X^T WX)^{-1}X^T WY$$\nwhich is the Weighted Least Squares (WLS) that can be solved by iterative algorithms. \n\n\\subsubsection{Statistical Test}\nAfter we fit the GLM for each voxel, we can perform a statistical test with the regressors we have. Usually, we construct a contrast vector $c$ that represent the null hypothesis, denoted as $H_0$. Then we conduct a t-test:\n$$t = \\frac{c^T\\beta^*}{\\sqrt{Var(c^T\\beta^*)}}$$\nWhere \n$$Var(c^T\\beta^*) = \\sigma^2 c^T(X^T X)^{-1}c$$\nwhere $\\sigma$ corresponds to the residual noise, and $(X^T X)^{-1}$ is the efficiency matrix as mentioned before.\n\\\\ \\\\\nThe $t$ score represent the relative likelihood of the null hypothesis to be rejected. It could also be F-test if we have contrast matrix, or we could calculate the p-values, z-values, or Fisher scoring (The choices of the statistical test depends on varies aspects of the performed experiment). We then reconstruct the brain images with the scores of each voxel, and setting threshold to visualize which area voxels are activated. \nQuality Control exists every where in fMRI project. Selecting appropriate threshold should be treat seriously. As shown in Figure 8, different setting in threshold will directly influence the conclusion of which areas of the brain are activated. Two types of errors are now revealed:\ni) False Positive, where $H_0$ is true, but we reject it, i.e. the voxel is not activated, but we determine it as activated.\nii) False Negative, where $H_0$ is false, but we failed to reject it, i.e. the voxel is activated, but we determine it as not activated. \n\\\\ \nSo basically, we are dealing with the tradeoffs between sensitivity (true positive rate) and specificity (true negative  rate). There are two main approaches to the tradeoffs:\ni) voxel based, i.e. seek in finding an upper-bound of threshold normalized by the number of voxel.\\\\\nii) Cluster-level Inference,  an intuitive way to filter out the peaks. \n\\\\ \\\\\nThere are numbers of algorithms in each approach. Here I only provide the most common used strategies.\n\\\\ \\\\\nA voxel based approach called False Discovery Rate (FDR) becomes popular recently that focus on the proportion of false positive rate among rejected tests. Benjamini-Hochberg is the most popular FDR algorithm:\\\\\ni) Rank our scores, with $p_1 \\le p_2 \\le ... \\le p_m$ where $m$ is the total number of voxels.\\\\\nii) find the largest $i$ such that $p_i \\le \\frac{i}{m}*q$ where $q$ is our selected desired limit (usually 0.05).  \niii) Finally, we reject all hypothesis corresponding to $p_1, p_2, ..., p_r$. \n\\\\ \\\\\nCluster-level Inference is more widely used than voxel based. Threshold Free Cluster Enhancement (TFCE) is the popular one, where we control the threshold by two hyper-parameters: $u_c$ and the area, as shown in Figure 9. \nThough many packages implemented these algorithms with some default values, it is still worth to manually check and manipulate for the proper threshold. \nThis type of connectivity seek to make inferences on the structure of the relationships among brain regions. In terms of graph theory, the relationships are often undirected graph. Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are widely used algorithm for studying function connectivity. Let's say we have matrix $X$ where each row represent the reshaped voxels at each time step. There are some underlying \"components\" in $X$ that represent the extent of association of different areas of the brain. PCA decompose the matrix $X$ in three matrix as shown in Figure 10, where each $S_{ii}U_i \\otimes V^T_i$ is a principal component, and $S_{ii}$ represent the extent of importance of this component. As we can see, each component has the same shape with $X$, so they are also the time series images, and can be visualized as shown in Figure 11.\n\nICA on the other hand, decompose matrix $X$ into two matrix, as shown in Figure 12, and we can visualize the components as we do with PCA, as shown in Figure 13. \n\nPCA assumes an orthogonality among the components, and ICA assumes statistical independence among the components. Usually, independence is a stronger requirement than orthogonality (which we can also see that ICA provide better visualization than PCA), but ICA didn't rank the components which PCA does.\nEffective connectivity makes stronger conclusions than functional connectivity. In terms of graph theory, the relationships often represented as directed graph. To study the effective connectivity, there two main approaches: mediation and modulation. \nAs shown in Figure 14, mediation is the study that seek in establish the relationships among factors, and moderation seek in finding the conditional relationships among factors. For example, we want to study the relationship between the activation of brain area $x$ and the heart rate. With mediation, we will explore what factor will cause the activation in area $x$, pain or enjoyment for instance, and come up with conclusion based on the experimental results. With modulation, if we assume that pain will activate area $x$, then we will change the extent of pain, and see if the correlation between the activation of area $x$ and heart rate still exists.\nBecause the traditional brain mapping uses mass-univariate analysis approach, every voxel is modeled separately and independently. There are no interaction information included, which is a core limitation of such approach. With more data becomes available, techniques in machine learning becomes applicable on fMRI data. MVPA as shown in Figure 10, seek in assessing whether a pattern across voxels predicts a behavior or outcome. This approach takes whole brain MRI or fMRI image(s) $X$ as input, and supervised by the output $Y$ that could be the conditions at next time steps (Regression), or the labels of either the behavioral or psychological states (Classification). \nWith regression tasks, here are some example choices:\ni) Linear Regression (e.g. Ridge, Lasso, etc.)\nii) K-Nearest-Neighbor (KNN)\niii) Decision Tree\n\\\\ \\\\\nWith classification tasks, the example choices are:\ni) Logistic Regression (Linear Classification with logistic loss function)\nii) Support Vector Machine (SVM)\\\\\niii) Neural Networks\n\\\\ \\\\\nEspecially for the choices of Neural Networks, since MRI data is essentially image data, Convolution Neural Networks could be an appropriate choices; and since fMRI data is essentially a time series data, Recurrent Neural Networks such as Long-Term-Short-Memory (LSTM) and Gated Recurrent Units (GRU) are worth to try. \n\\\\ \\\\\nFor the space concerning, details about these machine learning models will not be discussed here. If you are interested in these techniques, there are abundant online resources available. You could also read my other original articles about the applications in other fields such as:\ni) \\textit{Report: Short Term and Long Term Cases Forecasting of COVID-19 withVaries Neural Networks}, available at:\n\\url{https://yunfeiluo.com/share.html?article=articles/reports/covid_19_cases_forecasting.pdf}\nii) \\textit{Paper review: A Neural Algorithm of Artistic Style, by Gatys, Ecker, and Bethge}, available at: \n\\url{https://yunfeiluo.com/share.html?article=articles/blogs/paper_review_artistic_style.pdf}\n\n\\subsubsection{Model Evaluation}\nAs always, the quality control in this analysis mode is done by evaluating the performance of the model. For regression tasks, the performance of the model on test set are often Mean Square Error (MSE), or Mean Absolute Error (MAE). For classification tasks, the evaluating matrics are often F1-Score, or AUC-ROC. \n\\\\ \nFor splitting the training and testing set, k-fold cross validation is a widely used method. The procedure is:\ni) split data evenly into k folds (Could be stratified by the classes, in order to ensure evenly exposure of each class for training)\\\\\nii) for each fold $i \\in$ \\{1, 2, ..., k\\}, train model on the rest folds, and testing on fold $i$\\\\\niii) average the score (depends on the evaluating metric) across all the folds. \n\n\\{Epilogue}\n. Personally, I'm interested in applying the machine learning techniques into the analysis. I believe that the developing with reasonable algorithms and models are extreme beneficial to the clinical applications. I will keep learning and exploring in this direction."
=======
{
    "id": -1,
    "type": "blog",
    "title": "Introduction to functional Magnetic Resonance Imaging",
    "path": "articles/blogs/Intro_to_fMRI.pdf",
    "docs": [],
    "tags": [
        "Cognitive Science",
        "Machine Learning",
        "Psychology",
        "Physics",
        "Biology",
        "Neural Science"
    ],
    "summery": "With the advanced technologies of Magnetic Resonance Imaging (MRI) and Statistical Analysis, scientists are able to study human's brain in an appropriate way. This blog provides a clear and comprehensive representation of the bird-view of fMRI field based on my personal learning note of the course, Principles of fMRI 1 and 2}, offered by Johns Hopkins University and University of Colorado Boulder.",
    "text": "With the advanced technologies of Magnetic Resonance Imaging (MRI) and Statistical Analysis, scientists are able to study human's brain in an appropriate way. In the fields of Cognitive Science, Neural Science, and Psychology, studying the relationships between the brain activity and the behaviors or psychological states reveals great potentials in clinical applications. With the inclusion of techniques in Computer Science, the scientific studies becomes more effective, which can be revealed in the interdisciplinary field: functional Magnetic Resonance Imaging (fMRI), that dealing with series of MRI images across time. This blog provides a clear and comprehensive representation of the bird-view of fMRI field based on my personal learning note of the course \\textit{Principles of fMRI 1 \\& 2} offered by Johns Hopkins University and University of Colorado Boulder. \nThe final goals of fMRI can be summarized as:\n\\\\ \\\\\ni) \\textbf{Localization}: Determine which parts of the brain are activated during specific task(s).\n\\\\ \\\\\nii) \\textbf{Connectivity}: How regions are connected with one another. \n\\\\ \\\\\niii) \\textbf{Prediction}: Use brain activity to predict perceptions, behavior, or health status. \n\\\\ \\\\\nBefore we analyze toward the interests above, we first need to design an appropriate experiment to approach the tasks that we are interested in. Block Design and Event-related Design are the two main types of experimental design which will be discussed in details in the following sections. \n\\\\ \\\\\nThen we need to collect data, and analyze the data. The proper analyzing modes are Tradition Brain Mapping, which is a classical approach in science, and Multivoxel (or Multivariate) Pattern Analysis (MVPA) which is often related to techniques of Machine Learning. These modes will be discussed in details in the later sections. \n\\\\ \\\\\nMoreover, considering that there are many variability in experimental design, model selection, and the varies source of noises such as head movements, heart rate, and respiration, etc., the quality control is significant across the entire fMRI project as shown in Figure 1. Again, more details will be discussed in the later sections. \n\n\\section{Experimental Design}\nDuring the experiments, we want the subjects to do the tasks we want, and detect the effects based on the observations on the fMRI data of the brain. For example, we may want to see how brain responses if we ask the participants to look at images that could be either positive (make people feel comfortable, could be happy or enjoyment) or negative (make people feel bad, such as sad and pain). The question is now revealed: how should we arrange the order of tasks that exposes to or done by the subjects in order to effectively detect the contrasts or responses from the brain activity? The two main strategies are Block-Design and Event-Related-Design, as shown in Figure 2. \n\n\\subsection{Block Design}\nIn Block design, the similar events are grouped into a single block. The length of the block is fixed once set during the experiment. \n\\\\ \\\\\nEach blocks have the same length, often about 16 to 20 seconds. If the block length is too short, there might not have enough time space for the brain to show differences among different tasks, which often caused by vascular inelasticity. If use long blocks, more than 40 seconds for example, it is risky that there will be more unwanted activation caused by the out-of-scope association and imagination done by the subjects. \n\n\\subsection{Event-Related Design}\nIn this design mode, the tasks are mixed in orders, with unequal length. \n\\\\ \\\\\nFor this kind of design, there are no strict requirements on the boundaries of the events' length, but if there are low-frequency components, such as a long gap between two events, it is still risky with the same reasons as mentioned in Block-Design. \nThe Rule-of Thumb is to have 30 to 40 minutes of functional scan time for each subjects considering the psychological effects such as fatigue and habituation. And scan as many participants as possible for better generalization and robustness of the study. \n\\\\ \\\\\nThe tradeoffs among different factors always exist. The block design is robust in detect the contrast of responses of brain among different tasks. However, the blocks always comes in same length and same order. Considering the effects of Neural Habituation, i.e. the subjects will know what will be the next block, and will be subconsciously prepared for that, the contrast results is not sufficient for making specific psychological or behavioral inferences. On the other hand, although event-related design might not provide a clear contrast in responses among the tasks, it has better estimation to the shape of hemodynamic responses which is significant for making specific inferences. \n\\\\ \\\\\nIn terms of efficiency, having 2 conditions (2 different tasks) is optimal in either design mode. And the experimental efficiency has order: \\\\\nBlock-Design $>$ Dense-Event-Related-Design $>$ Sparse-Event-Related-Design\n\\\\ \\\\\nIn the cases where the number of conditions $\\ge 2$, Genetic Algorithm (GA) is often used for searching in the random space of the order of tasks exposes to the subjects. Details of GA will not be discussed here. Basically an MRI images consist of number of voxels with their value of intensity, which can also be interpret as the pixels in 3 dimensions. So in fMRI, we have a time series data for each voxel as shown in Figure 4. \n\n\\subsection{Data Acquisition}\nPhysically, the data is collected by controlling the motion of nuclei of hydrogen atoms of the subjects. Each nuclei is spinning at some directions, and has its own magnetic field. When the subjects is placed in the Magnetic Resonance (MR) scanner, the nuclei will align with the magnetic field of the equipment and form an \\textbf{Longitudinal Magnetization}. The nuclei now have the same directions of spinning, but could have different phases. \n\\\\ \\\\\nThen during the scanning, a Radio Frequency (RF) pulse is performed. The phases of the nuclei will be aligned, and will be tipped over and form an \\textbf{Transversal Magnetization}. The degree for such tip-over is often 90 degrees. \n\\\\ \\\\\nThen we remove the RF pulse, and the transversal Magnetization will decrease exponentially, and longitudinal magnetization will increase exponentially back to its original size. The signal created during this process will then be catched by a receiver coil. The signal is then map to a image as shown in Figure 3 by Fourier Transfer. \n\n\\subsection{Blood Oxygenation Level Dependent (BOLD)}\nStudying oxygenation changes in the brain across time is the most common approach in fMRI. It measures the ratio of oxygenated to deoxygenated hemoglobin in the blood. Because oxyhemoglobin is diamagnetic and deoxyhemoglobin is paramagnetic, their difference in magnetism allow us to catch the signal of how these two types of hemoglobin distributed and changes in the brain. \n\\\\ \\\\\nSince when the neurons are activated, they will absorb oxyhemoglobin from the neighbor blood vessels, we doesn't measure the neural activity directly, but metabolic demands (oxygen consumption) of the active neurons. As the neurons absorb oxyhemoglobin, there will be a quick decrease in oxyhemoglobin and increase in deoxyhemoglobin, which will be shown as an decreases of magnetic resonance signal. But suddenly, as the body will sending more oxyhemoglobin to reply the oxygen consumption, there will be a significant increase in oxyhemoglobin and decrease in deoxyhemoglobin, which will result in more magnetic resonance signal. After about 4 to 6 seconds, the intensity of the signal will reach the peak, and start reduces below the baseline, then go back to the original situations. The entire process demostrate an delayed responses and is quite slow that it would take about 20 to 30 seconds. The signal vs. time wave is known as Hemodynamic Response Function (HRF), which we will discuss more about it in the section of analysis. \n\n\\subsection{Noise \\& Registration}\nThere are always noise during the scanning, could be artifacts or errors that are inevitable. Head motion, heart beat, and respiration are often great concerns in fMRI. In order to dealing with such noise, an Intrasubject registration is needed, where we need to ensure that each voxels across time always represent the same area. However, the motion often influence the the magnetic field which will leave a \"spin-history\" of the nuclei, which cannot be entirely removed. \n\\\\ \\\\\nMoreover, for the purpose of robustness and generalization of the study, a spatial normalization is needed, which often refer to as Intersubject registration. One subject will often be selected as the \"standard\", and the data from other subjects will be manipulated to align with the standard. Such normalization allows researchers to come up with the results making sense across the population. \n\n\\subsection{Quality Control}\nAs we mentioned in the previous section, there are always tradeoffs that need to deal with. Echo-Planer Imaging (EPI), a standard and widely used techniques for obtaining functional images, collects images slice-by-slice from bottom to top of the brain. Figure 5 briefly shows the tradeoff relationships of EPI among brain coverage, Spatial and Temporal resolution, and susceptibility artifacts. \nFirstly, more coverage of the brain will help a lot in registration and normalization, but there will be less temporal resolution and more susceptibility artifacts. Secondly, although higher resolution often reduce the susceptibility artifacts, spatial and temporal resolution always cannot be compatible. Higher spatial resolution lead to better decoding and prediction, especially in small regions activity. But there are cost in temporal resolution. Higher temporal resolution has better ability to separate and remove artifacts but has cost in spatial resolution, time, and coverage. Finally, higher susceptibility artifacts will provide better fidelity, especially localization in problematic brain areas.\nUsually, the analyzing tasks use mass univariate approach, which is also known as traditional brain mapping, as shown in Figure 6. In this approach we modeling for each voxel separatly and independently.\nFirst of all, we will fit a General Linear Model (GLM) for each voxel as shown in Figure 7 with: \n$$Y=X\\beta+\\epsilon$$\nWhere $Y$ is the observed time series data of the single voxel. $X$ is the design matrix, which is often the assumed canonical HRF, or matrix that contain canonical HRF, its derivative, and its dispersion derivative. Finite Impulse Response (FIR) is also a proper choices for the design matrix. $\\beta$ is the regressors, and $\\epsilon$ is the error term. By perform the fitting task, we are solving the optimization problem:\n$$min_{\\beta} \\;\\; ||Y-X\\beta||_2^2$$\nWe can easily solve this problem by taking the derivative with respect to $\\beta$ and set it to zero. We finally arrive the Ordinary Least Squares (OLS):\n$$\\beta^* = (X^T X)^{-1}X^T Y$$\nWhere the diagnal of the matrix $(X^T X)^{-1}$ is defined as the design efficiency, which is a significant metric to evaluate the tradeoffs. However, OLS is only optimal if the error or the noise term $\\epsilon$ is i.i.d (Independent and Identical Distributed). Otherwise, we will introduce a weighted matrix $W$ that corresponds to the actual variance:\n$$\\beta^* = (X^T WX)^{-1}X^T WY$$\nwhich is the Weighted Least Squares (WLS) that can be solved by iterative algorithms. \n\n\\subsubsection{Statistical Test}\nAfter we fit the GLM for each voxel, we can perform a statistical test with the regressors we have. Usually, we construct a contrast vector $c$ that represent the null hypothesis, denoted as $H_0$. Then we conduct a t-test:\n$$t = \\frac{c^T\\beta^*}{\\sqrt{Var(c^T\\beta^*)}}$$\nWhere \n$$Var(c^T\\beta^*) = \\sigma^2 c^T(X^T X)^{-1}c$$\nwhere $\\sigma$ corresponds to the residual noise, and $(X^T X)^{-1}$ is the efficiency matrix as mentioned before.\n\\\\ \\\\\nThe $t$ score represent the relative likelihood of the null hypothesis to be rejected. It could also be F-test if we have contrast matrix, or we could calculate the p-values, z-values, or Fisher scoring (The choices of the statistical test depends on varies aspects of the performed experiment). We then reconstruct the brain images with the scores of each voxel, and setting threshold to visualize which area voxels are activated. \nQuality Control exists every where in fMRI project. Selecting appropriate threshold should be treat seriously. As shown in Figure 8, different setting in threshold will directly influence the conclusion of which areas of the brain are activated. Two types of errors are now revealed:\ni) False Positive, where $H_0$ is true, but we reject it, i.e. the voxel is not activated, but we determine it as activated.\nii) False Negative, where $H_0$ is false, but we failed to reject it, i.e. the voxel is activated, but we determine it as not activated. \n\\\\ \nSo basically, we are dealing with the tradeoffs between sensitivity (true positive rate) and specificity (true negative  rate). There are two main approaches to the tradeoffs:\ni) voxel based, i.e. seek in finding an upper-bound of threshold normalized by the number of voxel.\\\\\nii) Cluster-level Inference,  an intuitive way to filter out the peaks. \n\\\\ \\\\\nThere are numbers of algorithms in each approach. Here I only provide the most common used strategies.\n\\\\ \\\\\nA voxel based approach called False Discovery Rate (FDR) becomes popular recently that focus on the proportion of false positive rate among rejected tests. Benjamini-Hochberg is the most popular FDR algorithm:\\\\\ni) Rank our scores, with $p_1 \\le p_2 \\le ... \\le p_m$ where $m$ is the total number of voxels.\\\\\nii) find the largest $i$ such that $p_i \\le \\frac{i}{m}*q$ where $q$ is our selected desired limit (usually 0.05).  \niii) Finally, we reject all hypothesis corresponding to $p_1, p_2, ..., p_r$. \n\\\\ \\\\\nCluster-level Inference is more widely used than voxel based. Threshold Free Cluster Enhancement (TFCE) is the popular one, where we control the threshold by two hyper-parameters: $u_c$ and the area, as shown in Figure 9. \nThough many packages implemented these algorithms with some default values, it is still worth to manually check and manipulate for the proper threshold. \nThis type of connectivity seek to make inferences on the structure of the relationships among brain regions. In terms of graph theory, the relationships are often undirected graph. Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are widely used algorithm for studying function connectivity. Let's say we have matrix $X$ where each row represent the reshaped voxels at each time step. There are some underlying \"components\" in $X$ that represent the extent of association of different areas of the brain. PCA decompose the matrix $X$ in three matrix as shown in Figure 10, where each $S_{ii}U_i \\otimes V^T_i$ is a principal component, and $S_{ii}$ represent the extent of importance of this component. As we can see, each component has the same shape with $X$, so they are also the time series images, and can be visualized as shown in Figure 11.\n\nICA on the other hand, decompose matrix $X$ into two matrix, as shown in Figure 12, and we can visualize the components as we do with PCA, as shown in Figure 13. \n\nPCA assumes an orthogonality among the components, and ICA assumes statistical independence among the components. Usually, independence is a stronger requirement than orthogonality (which we can also see that ICA provide better visualization than PCA), but ICA didn't rank the components which PCA does.\nEffective connectivity makes stronger conclusions than functional connectivity. In terms of graph theory, the relationships often represented as directed graph. To study the effective connectivity, there two main approaches: mediation and modulation. \nAs shown in Figure 14, mediation is the study that seek in establish the relationships among factors, and moderation seek in finding the conditional relationships among factors. For example, we want to study the relationship between the activation of brain area $x$ and the heart rate. With mediation, we will explore what factor will cause the activation in area $x$, pain or enjoyment for instance, and come up with conclusion based on the experimental results. With modulation, if we assume that pain will activate area $x$, then we will change the extent of pain, and see if the correlation between the activation of area $x$ and heart rate still exists.\nBecause the traditional brain mapping uses mass-univariate analysis approach, every voxel is modeled separately and independently. There are no interaction information included, which is a core limitation of such approach. With more data becomes available, techniques in machine learning becomes applicable on fMRI data. MVPA as shown in Figure 10, seek in assessing whether a pattern across voxels predicts a behavior or outcome. This approach takes whole brain MRI or fMRI image(s) $X$ as input, and supervised by the output $Y$ that could be the conditions at next time steps (Regression), or the labels of either the behavioral or psychological states (Classification). \nWith regression tasks, here are some example choices:\ni) Linear Regression (e.g. Ridge, Lasso, etc.)\nii) K-Nearest-Neighbor (KNN)\niii) Decision Tree\n\\\\ \\\\\nWith classification tasks, the example choices are:\ni) Logistic Regression (Linear Classification with logistic loss function)\nii) Support Vector Machine (SVM)\\\\\niii) Neural Networks\n\\\\ \\\\\nEspecially for the choices of Neural Networks, since MRI data is essentially image data, Convolution Neural Networks could be an appropriate choices; and since fMRI data is essentially a time series data, Recurrent Neural Networks such as Long-Term-Short-Memory (LSTM) and Gated Recurrent Units (GRU) are worth to try. \n\\\\ \\\\\nFor the space concerning, details about these machine learning models will not be discussed here. If you are interested in these techniques, there are abundant online resources available. You could also read my other original articles about the applications in other fields such as:\ni) \\textit{Report: Short Term and Long Term Cases Forecasting of COVID-19 withVaries Neural Networks}, available at:\n\\url{https://yunfeiluo.com/share.html?article=articles/reports/covid_19_cases_forecasting.pdf}\nii) \\textit{Paper review: A Neural Algorithm of Artistic Style, by Gatys, Ecker, and Bethge}, available at: \n\\url{https://yunfeiluo.com/share.html?article=articles/blogs/paper_review_artistic_style.pdf}\n\n\\subsubsection{Model Evaluation}\nAs always, the quality control in this analysis mode is done by evaluating the performance of the model. For regression tasks, the performance of the model on test set are often Mean Square Error (MSE), or Mean Absolute Error (MAE). For classification tasks, the evaluating matrics are often F1-Score, or AUC-ROC. \n\\\\ \nFor splitting the training and testing set, k-fold cross validation is a widely used method. The procedure is:\ni) split data evenly into k folds (Could be stratified by the classes, in order to ensure evenly exposure of each class for training)\\\\\nii) for each fold $i \\in$ \\{1, 2, ..., k\\}, train model on the rest folds, and testing on fold $i$\\\\\niii) average the score (depends on the evaluating metric) across all the folds. \n\n\\{Epilogue}\n. Personally, I'm interested in applying the machine learning techniques into the analysis. I believe that the developing with reasonable algorithms and models are extreme beneficial to the clinical applications. I will keep learning and exploring in this direction."
>>>>>>> origin
  }